{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 2 deberes TFG\n",
    "- Utilizando todo el ss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from baro.anomaly_detection import bocpd\n",
    "from baro.root_cause_analysis import robust_scorer\n",
    "from baro.utility import drop_constant\n",
    "from sklearn import datasets\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to load data from multiple directories\n",
    "def load_data_from_dirs(base_dir, sub_dirs, file_name):\n",
    "    data_frames = []\n",
    "    for sub_dir in sub_dirs:\n",
    "        full_path = os.path.join(base_dir, sub_dir)\n",
    "        for i in range(1, 6):\n",
    "            file_path = os.path.join(full_path, str(i), file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                df_temp = pd.read_csv(file_path)\n",
    "                df_temp['file_origin'] = sub_dir\n",
    "                data_frames.append(df_temp)\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Base directory and subdirectories\n",
    "base_dir = '.'\n",
    "sub_dirs = [\n",
    "    'carts_cpu', 'carts_delay', 'carts_loss', 'carts_mem',\n",
    "    'catalogue_cpu', 'catalogue_delay', 'catalogue_loss', 'catalogue_mem',\n",
    "    'orders_cpu', 'orders_delay', 'orders_loss', 'orders_mem',\n",
    "    'payment_cpu', 'payment_delay', 'payment_loss', 'payment_mem',\n",
    "    'user_cpu', 'user_delay', 'user_loss', 'user_mem'\n",
    "]\n",
    "\n",
    "# Load data\n",
    "df = load_data_from_dirs(base_dir, sub_dirs, 'new_data.csv')\n",
    "\n",
    "# Display the number of rows and value counts\n",
    "print(\"Número de filas en el DataFrame original:\", df.shape[0])\n",
    "print(df[['Abnormality Class', 'file_origin']].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Asignar la clase de anomalía dependiendo del archivo de origen\n",
    "def assign_anomaly_class(row):\n",
    "    if row['Abnormality Class'] == 'Abnormal':\n",
    "        if row['file_origin'] == 'carts_cpu':\n",
    "            return 'CPU HOG'\n",
    "        elif row['file_origin'] == 'carts_mem':\n",
    "            return 'MEM LEAK'\n",
    "        elif row['file_origin'] == 'carts_loss':\n",
    "            return 'Packet Loss'\n",
    "        elif row['file_origin'] == 'carts_delay':\n",
    "            return 'Packet Delay'\n",
    "        elif row['file_origin'] == 'catalogue_cpu':\n",
    "            return 'CPU HOG'\n",
    "        elif row['file_origin'] == 'catalogue_mem':\n",
    "            return 'MEM LEAK'\n",
    "        elif row['file_origin'] == 'catalogue_loss':\n",
    "            return 'Packet Loss'\n",
    "        elif row['file_origin'] == 'catalogue_delay':\n",
    "            return 'Packet Delay'\n",
    "        elif row['file_origin'] == 'orders_cpu':\n",
    "            return 'CPU HOG'\n",
    "        elif row['file_origin'] == 'orders_mem':\n",
    "            return 'MEM LEAK'\n",
    "        elif row['file_origin'] == 'orders_loss':\n",
    "            return 'Packet Loss'\n",
    "        elif row['file_origin'] == 'orders_delay':\n",
    "            return 'Packet Delay'\n",
    "        elif row['file_origin'] == 'payment_cpu':\n",
    "            return 'CPU HOG'\n",
    "        elif row['file_origin'] == 'payment_mem':\n",
    "            return 'MEM LEAK'\n",
    "        elif row['file_origin'] == 'payment_loss':\n",
    "            return 'Packet Loss'\n",
    "        elif row['file_origin'] == 'payment_delay':\n",
    "            return 'Packet Delay'\n",
    "        elif row['file_origin'] == 'user_cpu':\n",
    "            return 'CPU HOG'\n",
    "        elif row['file_origin'] == 'user_mem':\n",
    "            return 'MEM LEAK'\n",
    "        elif row['file_origin'] == 'user_loss':\n",
    "            return 'Packet Loss'\n",
    "        elif row['file_origin'] == 'user_delay':\n",
    "            return 'Packet Delay'\n",
    "    return row['Abnormality Class']\n",
    "\n",
    "df['Abnormality Class'] = df.apply(assign_anomaly_class, axis=1)\n",
    "\n",
    "\n",
    "# Verificar los cambios\n",
    "print(df[['Abnormality Class']].value_counts())\n",
    "\n",
    "# Eliminar la columna 'file_origin'\n",
    "if 'file_origin' in df.columns:\n",
    "    df.drop(['file_origin'], axis=1, inplace=True)\n",
    "#print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with Abnormality Class == 'Normal'\n",
    "df_normal = df[df['Abnormality Class'] == 'Normal']\n",
    "\n",
    "# DataFrame with the rest of the Abnormality Classes\n",
    "df_abnormal = df[df['Abnormality Class'] != 'Normal']\n",
    "\n",
    "# Display the shapes of the new DataFrames\n",
    "print(\"Shape of df_normal:\", df_normal.shape)\n",
    "print(\"Shape of df_abnormal:\", df_abnormal.shape)\n",
    "null_count1 = df_normal.isnull().sum().sum()\n",
    "print('Number of null values in df_normal:', null_count1)\n",
    "null_count2 = df_abnormal.isnull().sum().sum()\n",
    "print('Number of null values in df_abnormal:', null_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for the abnormal classes\n",
    "abnormal_mapping = {\n",
    "    'CPU HOG': 2.1,\n",
    "    'MEM LEAK': 2.2,\n",
    "    'Packet Loss': 2.3,\n",
    "    'Packet Delay': 2.4\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Abnormality Class' column\n",
    "df['Abnormality Class'] = df['Abnormality Class'].replace(abnormal_mapping)\n",
    "df['Abnormality Class'] = df['Abnormality Class'].replace('Normal', 1)\n",
    "\n",
    "# Verify the changes\n",
    "print(df['Abnormality Class'].value_counts())\n",
    "\n",
    "df.to_csv('./processed_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "import seaborn as sns\n",
    "\n",
    "# 3. Imputación de valores nulos (si es necesario)\n",
    "# Create an instance of KNNImputer with 3 neighbors to fill missing values\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "# Apply the imputer to the DataFrame and create a new DataFrame with imputed values\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# 4. Asignar las características (X) y la etiqueta (y)\n",
    "# Separate features (X) and target label (y) from the imputed DataFrame\n",
    "X = df_imputed.drop('Abnormality Class', axis=1)\n",
    "y = df_imputed['Abnormality Class']\n",
    "\n",
    "# 5. Dividir los datos en conjunto de entrenamiento y validación (80% - 20%)\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "\n",
    "\n",
    "# Round and convert the target labels to integers\n",
    "y_train = y_train.round().astype(int)\n",
    "y_validation = y_validation.round().astype(int)\n",
    "\n",
    "# 6. Definir los modelos a evaluar\n",
    "# Define a list of models to evaluate\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))  # Logistic Regression\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))  # Linear Discriminant Analysis\n",
    "models.append(('KNN', KNeighborsClassifier()))  # K-Nearest Neighbors Classifier\n",
    "models.append(('CART', DecisionTreeClassifier()))  # Decision Tree Classifier\n",
    "models.append(('NB', GaussianNB()))  # Gaussian Naive Bayes\n",
    "models.append(('SVM', SVC()))  # Support Vector Machines\n",
    "\n",
    "# 7. Evaluación de los modelos usando validación cruzada\n",
    "# Evaluate the models using cross-validation\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')\n",
    "\n",
    "# 8. Comparar los modelos\n",
    "# Compare the models using a box plot\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Comparación de Modelos')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 9. Entrenar el modelo seleccionado (Regresión Logística en este caso)\n",
    "# Train the selected model (Logistic Regression in this case)\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "#model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 10. Hacer predicciones con el conjunto de validación\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(X_validation)\n",
    "predictions = predictions.round().astype(int)\n",
    "\n",
    "\n",
    "# 11. Evaluar las predicciones\n",
    "# Evaluate the predictions\n",
    "print(\"Exactitud:\", accuracy_score(y_validation, predictions))\n",
    "print(\"Matriz de Confusión:\\n\", confusion_matrix(y_validation, predictions))\n",
    "print(\"Reporte de Clasificación:\\n\", classification_report(y_validation, predictions))\n",
    "\n",
    "\n",
    "# 12. Visualizar la matriz de confusión\n",
    "# Visualize the confusion matrix\n",
    "cm = confusion_matrix(y_validation, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'CPU HOG','MEM LEAK','Packet Loss','Packet Delay'], yticklabels=['Normal', 'CPU HOG','MEM LEAK','Packet Loss','Packet Delay'])\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de Modelos\n",
    "\n",
    "In this section, the code evaluates and compares different machine learning models using cross-validation:\n",
    "\n",
    "Define Models: A list of models is created, including Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors, Decision Tree, Gaussian Naive Bayes, and Support Vector Machines.\n",
    "\n",
    "Cross-Validation: Each model is evaluated using 10-fold cross-validation. This means the training data is split into 10 parts, and the model is trained on 9 parts and tested on the remaining part. This process is repeated 10 times, each time with a different part as the test set.\n",
    "\n",
    "Store Results: The accuracy results for each model are stored in a list.\n",
    "\n",
    "Print Results: The mean and standard deviation of the accuracy for each model are printed.\n",
    "\n",
    "Box Plot: A box plot is created to visually compare the performance of the models. The box plot shows the distribution of accuracy scores for each model, allowing you to see which models perform better and how consistent their performance is.\n",
    "\n",
    "## Matriz de Confusión\n",
    "\n",
    "In this section, the code evaluates the performance of the selected model (Logistic Regression) on the validation set using a confusion matrix:\n",
    "\n",
    "Train Model: The Logistic Regression model is trained on the training data.\n",
    "\n",
    "Make Predictions: The trained model is used to make predictions on the validation set.\n",
    "\n",
    "Evaluate Predictions: The predictions are evaluated using various metrics:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances.\n",
    "Confusion Matrix: A table that shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "Classification Report: A detailed report that includes precision, recall, and F1-score for each class.\n",
    "Visualize Confusion Matrix: The confusion matrix is visualized using a heatmap. The heatmap shows the counts of true positives, true negatives, false positives, and false negatives, making it easy to see where the model is making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "\"\"\"\n",
    "This script performs the following tasks:\n",
    "1. Select relevant metrics (latency and errors):\n",
    "    - Assigns a time index to the dataframe.\n",
    "    - Filters columns containing \"latency-50\" or \"error\".\n",
    "    - Removes constant columns.\n",
    "    - Inserts a time column at the beginning of the dataframe.\n",
    "2. Anomaly detection with Multivariate BOCPD:\n",
    "    - Applies BOCPD to detect anomalies in the selected metrics.\n",
    "    - Prints the first 5 timestamps of detected anomalies.\n",
    "3. Visualize anomaly detection:\n",
    "    - Normalizes the data for easier visualization.\n",
    "    - Plots the selected metrics.\n",
    "    - Marks the first detected anomaly with a red dashed line.\n",
    "    - Displays the plot.\n",
    "4. Root cause analysis with BARO:\n",
    "    - Applies root cause analysis to identify metrics most related to the failure.\n",
    "    - Prints the top 10 metrics identified as the root cause.\n",
    "\"\"\"\n",
    "# SECCIÓN DE EVALUACIÓN CON BARO\n",
    "# ========================================\n",
    "\n",
    "# 1. Seleccionar métricas relevantes (latencia y errores)\n",
    "time_col = pd.Series(range(df.shape[0]))  # Asignar un índice de tiempo\n",
    "selected_cols = [c for c in df.columns if \"latency-50\" in c or \"error\" in c]  # Filtrar métricas\n",
    "selected_df = drop_constant(df[selected_cols])  # Eliminar columnas constantes\n",
    "selected_df.insert(0, \"time\", time_col)  # Insertar columna de tiempo\n",
    "\n",
    "# 2. Detección de anomalías con Multivariate BOCPD\n",
    "anomalies = bocpd(selected_df)  # Aplicar BOCPD para detectar anomalías\n",
    "print(\"Anomalías detectadas en los siguientes timestamps:\", anomalies[:5])  # Imprimir los primeros 5 timestamps de anomalías detectadas\n",
    "\n",
    "# 3. Visualizar la detección de anomalías\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalizar datos para facilitar la visualización\n",
    "for c in selected_df.columns:\n",
    "    if c != \"time\":\n",
    "        selected_df[c] /= selected_df[c].max()  # Normalizar cada columna excepto la columna de tiempo\n",
    "\n",
    "# Graficar métricas\n",
    "selected_df.drop(\"time\", axis=1).plot(figsize=(15, 5), title=\"Latencia y Error Rate\", alpha=0.5, legend=None)\n",
    "# Marcar las primeras 5 anomalías detectadas con líneas rojas\n",
    "for anomaly_time in anomalies[:5]:\n",
    "    plt.axvline(x=anomaly_time, color='r', linestyle='--')\n",
    "plt.show()  # Mostrar el gráfico\n",
    "\n",
    "# Añadir una leyenda para explicar el significado de cada color\n",
    "plt.figure(figsize=(10, 1))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "labels = selected_df.columns[1:]  # Excluir la columna de tiempo\n",
    "for color, label in zip(colors, labels):\n",
    "    plt.plot([], [], color=color, label=label)  # Crear líneas vacías para la leyenda\n",
    "plt.legend(loc='center', ncol=5, frameon=False)\n",
    "plt.axis('off')  # Ocultar los ejes\n",
    "plt.title('Leyenda de colores para las métricas')\n",
    "plt.show()\n",
    "\n",
    "# 4. Análisis de causa raíz con BARO\n",
    "ranks = robust_scorer(df, anomalies=anomalies)[\"ranks\"]  # Aplicar el análisis de causa raíz y obtener las métricas más relacionadas con el fallo\n",
    "print(\"Principales métricas relacionadas con el fallo:\")  # Imprimir las métricas principales\n",
    "print(ranks[:10])  # Mostrar las 10 principales métricas identificadas como causa raíz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates anomalies and performs Root Cause Analysis (RCA) using BARO on the dataset. It consists of four main steps:\n",
    "\n",
    "1️⃣ Selecting Relevant Metrics:\n",
    "\n",
    "Extracts only latency and error rate metrics since they are key indicators of failures.\n",
    "Removes constant columns to keep only useful data.\n",
    "Adds a time index to structure the data for analysis.\n",
    "\n",
    "2️⃣ Anomaly Detection with BOCPD:\n",
    "\n",
    "Applies Multivariate Bayesian Online Change Point Detection (BOCPD) to detect anomalies.\n",
    "Prints the first detected anomaly timestamps.\n",
    "\n",
    "3️⃣ Visualizing Anomalies:\n",
    "\n",
    "Normalizes the selected metrics for easier visualization.\n",
    "Plots the time series data of the latency and error metrics.\n",
    "Marks the first detected anomaly with a red vertical line in the plot.\n",
    "\n",
    "4️⃣ Root Cause Analysis (RCA) with BARO:\n",
    "\n",
    "Uses robust_scorer to analyze which metrics are the most related to the failure.\n",
    "Prints the top 10 root cause metrics, helping identify the service or component responsible for the failure.\n",
    "Key Insights:\n",
    "\n",
    "✅ This code automates anomaly detection and RCA using state-of-the-art Bayesian techniques.\n",
    "\n",
    "✅ The graph helps visualize when the failure occurs and how metrics behave.\n",
    "\n",
    "✅ The ranked list of root causes is crucial for diagnosing failures in microservices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
